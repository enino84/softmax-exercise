{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM9fOlydknpo"
      },
      "source": [
        "# üìå Building Your First Neural Network in Python: Softmax Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBegHFMmVTY"
      },
      "source": [
        "## üéØ Objective\n",
        "\n",
        "In this exercise, you will implement a **softmax** layer, a crucial component in neural networks used for classification tasks. The softmax function converts raw scores (logits) into probabilities, ensuring that the outputs sum to 1. This exercise is designed to assess your understanding of activation functions, vectorized computation, and numerical stability techniques.\n",
        "\n",
        "By completing this exercise, you will demonstrate your ability to:\n",
        "\n",
        "‚úÖ Implement the softmax transformation correctly.\n",
        "\n",
        "‚úÖ Ensure numerical stability in computations.\n",
        "\n",
        "‚úÖ Use efficient NumPy operations rather than loops.\n",
        "\n",
        "\n",
        "## üìù Instructions\n",
        "\n",
        "1.  Implement the function softmax(x):\n",
        "    - Input: A NumPy array x containing raw scores (logits).\n",
        "    - Output: A NumPy array of the same shape, where each value represents a probability.\n",
        "    - Key requirement: Your implementation should be numerically stable (preventing overflow errors).\n",
        "2. Use the provided function template below to complete the implementation.\n",
        "3. Run the unit tests to check if your implementation is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAUpce4DnNZr"
      },
      "source": [
        "### üìëFunction template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TarNRl-yjfLY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute the softmax of vector x.\n",
        "\n",
        "    Parameters:\n",
        "        x (numpy array): Input vector\n",
        "\n",
        "    Returns:\n",
        "        numpy array: Softmax probabilities\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # Apply the softmax transformation\n",
        "    # Tip: Consider numerical stability using max subtraction\n",
        "    ### END CODE HERE ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZHKgJVRlbpy"
      },
      "source": [
        "### üîß Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Q390rslUWJ",
        "outputId": "e1e3747b-9f39-49ad-b273-269cfa695043"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def test_softmax():\n",
        "    try:\n",
        "        # Test 0: Check if function returns None\n",
        "        test_input = np.array([2.0, 1.0, 0.1])\n",
        "        output = softmax(test_input)\n",
        "\n",
        "        if output is None:\n",
        "            raise AssertionError(\"‚ùå Test 0 Failed! The function softmax() returned None. Did you forget to implement it?\")\n",
        "\n",
        "        # Test 1: Check if the return type is a NumPy array\n",
        "        if not isinstance(output, np.ndarray):\n",
        "            raise AssertionError(f\"‚ùå Test 1 Failed! Expected a NumPy array, but got {type(output)} instead.\")\n",
        "\n",
        "        print(\"‚úÖ Test 1 Passed: Function returns a NumPy array.\")\n",
        "\n",
        "        # Test 2: Check if output shape matches input shape\n",
        "        if output.shape != test_input.shape:\n",
        "            raise AssertionError(f\"‚ùå Test 2 Failed! Expected output shape {test_input.shape}, but got {output.shape}.\")\n",
        "\n",
        "        print(\"‚úÖ Test 2 Passed: Output shape matches input shape.\")\n",
        "\n",
        "        # Test 3: Basic correctness\n",
        "        expected_output = np.exp(test_input - np.max(test_input)) / np.sum(np.exp(test_input - np.max(test_input)))\n",
        "        assert np.allclose(output, expected_output, atol=1e-6), f\"‚ùå Test 3 Failed! Expected {expected_output}, but got {output}.\"\n",
        "\n",
        "        print(\"‚úÖ Test 3 Passed: Softmax computed correctly for a simple input.\")\n",
        "\n",
        "        # Test 4: Sum of probabilities should be 1\n",
        "        sum_output = np.sum(output)\n",
        "        assert np.isclose(sum_output, 1.0, atol=1e-6), f\"‚ùå Test 4 Failed! The sum of softmax output should be 1, but got {sum_output}.\"\n",
        "\n",
        "        print(\"‚úÖ Test 4 Passed: Softmax probabilities sum to 1.\")\n",
        "\n",
        "        # Test 5: Numerical stability check\n",
        "        large_input = np.array([1000, 1000, 1000])\n",
        "        stable_output = np.array([1/3, 1/3, 1/3])\n",
        "        output_large = softmax(large_input)\n",
        "        assert np.allclose(output_large, stable_output, atol=1e-6), f\"‚ùå Test 5 Failed! Large values caused instability. Expected {stable_output}, but got {output_large}.\"\n",
        "\n",
        "        print(\"‚úÖ Test 5 Passed: Softmax handles large values correctly (numerical stability).\")\n",
        "\n",
        "        # Test 6: Handling negative values\n",
        "        negative_input = np.array([-3.0, -2.0, -1.0])\n",
        "        output_negative = softmax(negative_input)\n",
        "        assert np.isclose(np.sum(output_negative), 1.0, atol=1e-6), f\"‚ùå Test 6 Failed! Sum of probabilities should be 1 for negative input, but got {np.sum(output_negative)}.\"\n",
        "\n",
        "        print(\"‚úÖ Test 6 Passed: Softmax computed correctly for negative values.\")\n",
        "\n",
        "        print(\"\\nüéâ All tests passed successfully!\")\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "\n",
        "# Run the test function\n",
        "test_softmax()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4o7r1s4nfqB"
      },
      "source": [
        "---\n",
        "### ‚úÖ Solution: Implementing the Softmax Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lxSo4CJFoK3y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute the softmax of vector x in a numerically stable way.\n",
        "\n",
        "    Parameters:\n",
        "        x (numpy array): Input vector of raw scores (logits)\n",
        "\n",
        "    Returns:\n",
        "        numpy array: Softmax-transformed probabilities\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x))  # Avoid overflow by subtracting max(x)\n",
        "    return exp_x / np.sum(exp_x)   # Normalize to get probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-3XG7XorhT"
      },
      "source": [
        "### üìå Comments on the Solution\n",
        "1. Correctness\n",
        "\n",
        "    - The solution correctly implements the softmax function by applying the exponential transformation and normalizing the output so that the sum of probabilities equals 1.\n",
        "\n",
        "2. Numerical Stability\n",
        "\n",
        "    - The implementation includes a crucial stability enhancement:\n",
        "\n",
        "      ```python\n",
        "      np.exp(x - np.max(x))\n",
        "      ```\n",
        "      This prevents potential overflow issues when dealing with large input values.\n",
        "      Without this adjustment, np.exp(x) could result in inf (infinity) for large numbers, leading to incorrect results.\n",
        "\n",
        "3. Vectorization\n",
        "\n",
        "    - The implementation is fully vectorized using NumPy operations.\n",
        "    It does not use explicit loops, making it efficient and well-suited for handling large datasets.\n",
        "\n",
        "4. Alternative Implementations\n",
        "\n",
        "    - While this implementation is sufficient for most applications, a log-sum-exp trick can be used for extreme numerical stability cases:\n",
        "\n",
        "      ```python\n",
        "      def softmax(x):\n",
        "          log_sum_exp = np.log(np.sum(np.exp(x - np.max(x))))\n",
        "          return np.exp(x - np.max(x) - log_sum_exp)\n",
        "      ```\n",
        "\n",
        "      This method ensures even greater numerical stability but is usually unnecessary unless working with highly sensitive calculations.\n",
        "\n",
        "  ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "In this exercise, you have successfully:\n",
        "\n",
        "‚úîÔ∏è Implemented the softmax function from scratch.\n",
        "\n",
        "‚úîÔ∏è Ensured numerical stability for large values.\n",
        "\n",
        "‚úîÔ∏è Validated your function with comprehensive unit tests.\n",
        "\n",
        "‚úîÔ∏è Applied best practices in vectorized computation using NumPy.\n",
        "\n",
        "‚úÖ **Next Steps:** Now that you've completed this exercise, you are ready to:\n",
        "\n",
        "üöÄ Integrate softmax into a neural network.\n",
        "\n",
        "üîé Explore how softmax is used in classification tasks.\n",
        "\n",
        "üí° Experiment with different activation functions for deeper understanding.\n",
        "\n",
        "Keep up the great work! üéâ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
